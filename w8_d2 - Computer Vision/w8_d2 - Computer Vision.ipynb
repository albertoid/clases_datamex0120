{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Captura por webcam y region de interes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captura webcam\n",
    "\n",
    "def captura():\n",
    "    cam=cv2.VideoCapture(0)\n",
    "    while 1:\n",
    "        ret, frame=cam.read()\n",
    "        cv2.imshow('Captura', frame)\n",
    "        \n",
    "        if ret==False: break\n",
    "        \n",
    "        key=cv2.waitKey(1)\n",
    "        \n",
    "        if key%256==27: break # tecla ESC\n",
    "        \n",
    "        elif key%256==32: # SPACE\n",
    "            img_name='captura.png'\n",
    "            cv2.imwrite(img_name, frame)\n",
    "            print ('Captura guardada')\n",
    "            break\n",
    "            \n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contraste(): # pasa a blanco y negro puro\n",
    "    image=cv2.imread('captura.png')\n",
    "    im=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    im=(255-im)\n",
    "    umbral=170\n",
    "    \n",
    "    img=np.zeros(shape=im.shape)\n",
    "    for i in range(im.shape[0]):\n",
    "        for j in range(im.shape[1]):\n",
    "            if im[i][j]>umbral: img[i][j]=255\n",
    "            else: img[i][j]=0\n",
    "    \n",
    "    cv2.imshow('img', img)\n",
    "    cv2.imwrite('b&w.png', img)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contorno(): # regiones de interes\n",
    "    umbral_fino=10\n",
    "    image=cv2.imread('b&w.png')\n",
    "    im=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    im=(255-im)\n",
    "    thresh=cv2.adaptiveThreshold(im, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                 cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    rect_kernel=cv2.getStructuringElement(cv2.MORPH_RECT, (30,10))\n",
    "    \n",
    "    threshed=cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, rect_kernel)\n",
    "    \n",
    "    contorno,_=cv2.findContours(threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    s_contorno=sorted(contorno, key=lambda x: cv2.boundingRect(x)[1]+cv2.boundingRect(x)[0]*image.shape[1])\n",
    "    \n",
    "    idx=0\n",
    "    for cut in s_contorno:\n",
    "        idx+=1\n",
    "        x,y,w,h=cv2.boundingRect(cut)\n",
    "        roi=im[y:y+h, x:x+w]\n",
    "        \n",
    "        if h<umbral_fino or w<umbral_fino: continue\n",
    "            \n",
    "        cv2.imwrite(str(idx)+'.png', roi)\n",
    "        cv2.rectangle(im, (x,y), (x+w, y+h), (200, 0, 0), 2)\n",
    "        \n",
    "        cv2.imshow('Imagen B-N', im)\n",
    "        cv2.waitKey(0)\n",
    "    \n",
    "    return idx\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captura()\n",
    "contraste()\n",
    "print (contorno())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contorno()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captura()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintando\n",
    "\n",
    "img=cv2.imread('captura.png')\n",
    "\n",
    "H=img.shape[0]\n",
    "W=img.shape[1]\n",
    "\n",
    "cv2.line(img, (W//2, 0), (W//2, H), (255, 255, 255), 15)\n",
    "cv2.circle(img, (W//2, H//2), 512, (0,0,255), 10)\n",
    "\n",
    "cv2.imshow('pinturas', img)\n",
    "cv2.resizeWindow('pinturas', 600, 600)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mezclar imagenes\n",
    "\n",
    "img1=cv2.imread('captura.png', cv2.IMREAD_COLOR)\n",
    "img2=cv2.imread('b&w.png', cv2.IMREAD_COLOR)\n",
    "\n",
    "img2=cv2.resize(img2, img1.shape[:2][::-1], interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('adicion', cv2.add(img1, img2))\n",
    "cv2.resizeWindow('adicion', 600, 600)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento facial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "images={}\n",
    "\n",
    "for e in glob.glob('./fotos_alumnos/**/*.JPG'):\n",
    "    alumno=e.split('/')[2]\n",
    "    foto=e.split('/')[3]\n",
    "    \n",
    "    img=cv2.imread(e)\n",
    "    \n",
    "    print ('Read\"{}\" -> {}'.format(alumno, foto))\n",
    "    gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if alumno in images:\n",
    "        images[alumno]['images'].append(gray)\n",
    "        images[alumno]['name'].append(e)\n",
    "    else:\n",
    "        images[alumno]={'images':[gray], 'name':[e]}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: x[0], images.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade=cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs=images['Reynaldo']['images']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i,e in enumerate(imgs):\n",
    "    img=e.copy()\n",
    "    faces=face_cascade.detectMultiScale(img, 1.1, 4)\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 4)\n",
    "        break\n",
    "    plt.subplot(2,2,i+1)\n",
    "    (x,y,w,h)=faces[0]\n",
    "    crop=img[y:y+h, x:x+w]\n",
    "    plt.imshow(crop, cmap='gray')\n",
    "    if i>=3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for alumno, data in images.items():\n",
    "    for i,e in enumerate(data['images']):\n",
    "        img=e.copy()\n",
    "        faces=face_cascade.detectMultiScale(img, 1.1, 4)\n",
    "        (x,y,w,h)=faces[0]\n",
    "        crop=img[y:y+h, x:x+w]\n",
    "        \n",
    "        if 'crops' in images[alumno]:\n",
    "            images[alumno]['crops'].append(crop)\n",
    "        else:\n",
    "            images[alumno]['crops']=[crop]\n",
    "        \n",
    "        name='{}/caras/{}-{}.JPG'.format(os.getcwd(), alumno, i)\n",
    "        \n",
    "        print (name)\n",
    "        \n",
    "        cv2.imwrite(name, crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(img, n_f=40):\n",
    "    fft=np.log10(np.abs(np.fft.fft2(img)))\n",
    "    w=fft.shape[0]//2\n",
    "    fft=fft[0:w, 0:w]\n",
    "    feats=fft[:n_f, :n_f]\n",
    "    return np.hstack(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacols=[]\n",
    "for alumno, data in images.items():\n",
    "    for crop,name in zip(data['crops'], data['name']):\n",
    "        X_alu=features(crop)\n",
    "        datacols.append({'alumno':alumno, 'foto':name, 'features':X_alu})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(datacols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=df.groupby('alumno').nth(0).reset_index()\n",
    "X_train=df[~df['foto'].isin(X_test['foto'])].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=[]\n",
    "\n",
    "for e in X_test.index:\n",
    "    X_miss=X_test.iloc[e]['features']\n",
    "    X_miss_name=X_test.iloc[e]['alumno']\n",
    "    X_data=X_train['features']\n",
    "    \n",
    "    dist=[]\n",
    "    \n",
    "    for i,e in X_data.items():\n",
    "        if e.shape==X_miss.shape:\n",
    "            d=np.linalg.norm(e-X_miss)\n",
    "            dist.append((int(i), d))\n",
    "    \n",
    "    sorted_d=pd.DataFrame(dist, columns=['index', 'distance']).sort_values(by='distance')\n",
    "    w_idx=int(sorted_d.iloc[0].values[0])\n",
    "    p=X_train.iloc[w_idx]['alumno']\n",
    "    preds.append([X_miss_name, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preds, columns=['real', 'prediccion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deteccion de objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "THRESHOLD=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {0: 'background',\n",
    "              1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus',\n",
    "              7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant',\n",
    "              13: 'stop sign', 14: 'parking meter', 15: 'bench', \n",
    "               16: 'bird', 17: 'cat',\n",
    "              18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear',\n",
    "              24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella', 31: 'handbag',\n",
    "              32: 'tie', 33: 'suitcase', \n",
    "               34: 'frisbee', 35: 'skis', 36: 'snowboard',\n",
    "              37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove',\n",
    "              41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle',\n",
    "              46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon',\n",
    "              51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange',\n",
    "              56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut',\n",
    "              61: 'cake', 62: 'chair', \n",
    "               63: 'couch', 64: 'potted plant', 65: 'bed',\n",
    "              67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse',\n",
    "              75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven',\n",
    "              80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book', 85: 'clock',\n",
    "              86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=cv2.dnn.readNetFromTensorflow('frozen_inference_graph.pb', \n",
    "                                    'ssd_mobilenet_v2_coco_2018_03_29.pbtxt')\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while 1:\n",
    "    ret,frame=cap.read()\n",
    "    model.setInput(cv2.dnn.blobFromImage(frame, size=(300,300), swapRB=True))\n",
    "    output=model.forward()[0,0,:,:]\n",
    "    \n",
    "    for e in output:\n",
    "        confi=e[2]\n",
    "        if confi>THRESHOLD:\n",
    "            class_id=e[1]\n",
    "            class_name=class_names[class_id]\n",
    "            \n",
    "            box_x=e[3]\n",
    "            box_y=e[4]\n",
    "            box_w=e[5]q\n",
    "            box_h=e[6]\n",
    "            \n",
    "            h,w,ch=frame.shape\n",
    "            \n",
    "            box_x=e[3]*w\n",
    "            box_y=e[4]*h\n",
    "            box_w=e[5]*w\n",
    "            box_h=e[6]*h\n",
    "            \n",
    "            cv2.rectangle(frame, (int(box_x), int(box_y)), (int(box_w), int(box_h)), \n",
    "                          (0,255,0), thickness=2)\n",
    "            \n",
    "            cv2.putText(frame, class_name+' '+str(round(confi, 2)), (int(box_x), int(box_y+.05*h)),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0))\n",
    "            \n",
    "            cv2.imshow('deteccion', frame)\n",
    "            if cv2.waitKey(1) & 0xFF==ord('q'): break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO V3 (You Only Look Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "from yolo_utils import infer_image, show_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('-w', '--weights',\n",
    "        type=str,\n",
    "        default='./yolov3-coco/yolov3.weights',\n",
    "        help='')\n",
    "\n",
    "    parser.add_argument('-cfg', '--config',\n",
    "        type=str,\n",
    "        default='./yolov3-coco/yolov3.cfg',\n",
    "        help='Path to the configuration file for the YOLOv3 model.')\n",
    "\n",
    "    \n",
    "    parser.add_argument('-i', '--image-path',\n",
    "        type=str,\n",
    "        help='The path to the image file')\n",
    "\n",
    "    parser.add_argument('-v', '--video-path',\n",
    "        type=str,\n",
    "        help='The path to the video file')\n",
    "\n",
    "\n",
    "    parser.add_argument('-l', '--labels',\n",
    "        type=str,\n",
    "        default='./yolov3-coco/coco-labels',\n",
    "        help='Path to the file having the \\\n",
    "                    labels in a new-line seperated way.')\n",
    "\n",
    "    parser.add_argument('-c', '--confidence',\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help='The model will reject boundaries which has a \\\n",
    "                probabiity less than the confidence value. \\\n",
    "                default: 0.5')\n",
    "\n",
    "    \n",
    "    parser.add_argument('-th', '--threshold',\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help='The threshold to use when applying the \\\n",
    "                Non-Max Suppresion')\n",
    "\n",
    "    \n",
    "    parser.add_argument('--download-model',\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help='Set to True, if the model weights and configurations \\\n",
    "                are not present on your local machine.')\n",
    "\n",
    "    parser.add_argument('-t', '--show-time',\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help='Show the time taken to infer each image.')\n",
    "\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    \n",
    "    labels=open(FLAGS.labels).read().strip().split('\\n')\n",
    "    \n",
    "    colors=np.random.randint(0,255, size=(len(labels), 3), dtype='uint8')\n",
    "    \n",
    "    net=cv2.dnn.readNetFromDarknet(FLAGS.config, FLAGS.weights)\n",
    "    \n",
    "    layer_names=net.getLayerNames()\n",
    "    layer_names=[layer_names[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "    \n",
    "    if FLAGS.image_path:\n",
    "        print ()\n",
    "    \n",
    "    else:\n",
    "        count=0\n",
    "        vid=cv2.VideoCapture(0)\n",
    "        while 1:\n",
    "            ret,frame=vid.read()\n",
    "            h,w=frame.shape[:2]\n",
    "            if count==0:\n",
    "                frame, boxes, confi, c_ids, idxs= infer_image(net, layer_names, \n",
    "                                                             h,w,frame, colors, labels, FLAGS)\n",
    "                count+=1\n",
    "            else:\n",
    "                frame, boxes, confi, c_ids, idxs= infer_image(net, layer_names, \n",
    "                                                              h,w,frame, colors, labels, \n",
    "                                                              FLAGS, boxes, confi, c_ids, idxs,\n",
    "                                                              infer=False)\n",
    "                count=(count+1)%6\n",
    "            cv2.imshow('webcam', frame)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF==ord('q'): break\n",
    "    \n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
